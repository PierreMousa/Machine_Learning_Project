{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS #\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import feature\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_curve, auc,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# FEATURE EXTRACTION #\n",
    "#FEATURE EXTRACTION FUNCTION\n",
    "def extract_features(image):\n",
    "    hog_features, _ = feature.hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                                  cells_per_block=(2, 2), block_norm='L2-Hys', visualize=True)\n",
    "    return hog_features\n",
    "# DATA READING & LIST CREATION #\n",
    "# DATASET LOADING AND READING\n",
    "dataset_path = 'C:\\programing\\Ml project\\Dataset3'\n",
    "class_folders = os.listdir(dataset_path)\n",
    "\n",
    "# Lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "max_feature_length = 0  # Initialize variable to store maximum feature length\n",
    "\n",
    "# Loop through each class folder\n",
    "for class_folder in class_folders:\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    \n",
    "    # Loop through each image in the class folder\n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "        \n",
    "        # Extract HOG features\n",
    "        extracted_features = extract_features(image)\n",
    "        \n",
    "        # Update max_feature_length if the current feature length is greater\n",
    "        max_feature_length = max(max_feature_length, len(extracted_features))\n",
    "        \n",
    "        # Append features to the list\n",
    "        features.append(extracted_features)\n",
    "        \n",
    "        # Append the label to the list\n",
    "        labels.append(class_folder)\n",
    "# FEATURE RESIZING #\n",
    "# Resize features to a fixed size\n",
    "resized_features = [resize(feature, (max_feature_length,), mode='constant', anti_aliasing=True) for feature in features]\n",
    "features = np.array(resized_features)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "labels = np.array(labels)\n",
    "\n",
    "# USING ENCODERS #\n",
    "# Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_numeric = label_encoder.fit_transform(labels)\n",
    "\n",
    "# LIST PREPROCESSING #\n",
    "flattened_features = [np.array(feature).flatten() for feature in features]\n",
    "max_feature_length = max(len(feature) for feature in flattened_features)\n",
    "padded_features = pad_sequences(flattened_features, maxlen=max_feature_length, padding='post', truncating='post', dtype='float32')\n",
    "features = np.array(padded_features)\n",
    "\n",
    "# Initialize StratifiedKFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# PIPELINE #\n",
    "\n",
    "# Create a pipeline with StandardScaler and Logistic Regression (One-vs-Rest)\n",
    "pipeline = Pipeline([\n",
    "    ('variance_threshold', VarianceThreshold(threshold=0.0)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='lbfgs')))\n",
    "])\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "tpr_list = []\n",
    "fpr_list = []\n",
    "roc_auc_list = []\n",
    "\n",
    "\n",
    "# DATA SPLITING & CROSS VALIDATION #\n",
    "# REPORT #\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(features, labels_numeric), start=1):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = labels_numeric[train_index], labels_numeric[test_index]\n",
    "\n",
    "    # Fit the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # PREDICTION #\n",
    "    # Predictions on the testing set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Store metrics for this fold\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "    # ROC curve for each class\n",
    "    y_prob = pipeline.decision_function(X_test)\n",
    "    tpr_fold_list = []\n",
    "    fpr_fold_list = []\n",
    "    roc_auc_fold_list = []\n",
    "    for class_idx in range(len(np.unique(labels_numeric))):\n",
    "        # Convert labels to binary format\n",
    "        binary_labels = (y_test == class_idx).astype(int)\n",
    "\n",
    "        # Use predicted decision function for positive class\n",
    "        fpr, tpr, _ = roc_curve(binary_labels, y_prob[:, class_idx])\n",
    "        tpr_fold_list.append(tpr)\n",
    "        fpr_fold_list.append(fpr)\n",
    "\n",
    "        # Calculate AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_auc_fold_list.append(roc_auc)\n",
    "        print(f'Fold {fold}, Class {class_idx}, AUC: {roc_auc}')\n",
    "\n",
    "    tpr_list.append(tpr_fold_list)\n",
    "    fpr_list.append(fpr_fold_list)\n",
    "    roc_auc_list.append(roc_auc_fold_list)\n",
    "\n",
    "# Print average metrics over all folds\n",
    "print(f\"Average Accuracy: {np.mean(accuracies)}\")\n",
    "print(f\"Average Precision: {np.mean(precisions)}\")\n",
    "print(f\"Average Recall: {np.mean(recalls)}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores)}\")\n",
    "print(\"Average Confusion Matrix:\")\n",
    "print(np.mean(conf_matrices, axis=0))\n",
    "# Visualize the average confusion matrix\n",
    "plt.imshow(np.mean(conf_matrices, axis=0), interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Average Confusion Matrix')\n",
    "plt.colorbar()\n",
    "classes = label_encoder.classes_\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "for class_idx, class_name in enumerate(np.unique(labels)):\n",
    "    plt.figure()\n",
    "\n",
    "    # ROC curve for each fold\n",
    "    for fold, (tpr_fold, fpr_fold) in enumerate(zip(tpr_list, fpr_list), start=1):\n",
    "        plt.plot(fpr_fold[class_idx], tpr_fold[class_idx], lw=1, label=f'Fold {fold}')\n",
    "\n",
    "    # Plot the average ROC curve\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.mean([np.interp(mean_fpr, fpr_fold[class_idx], tpr_fold[class_idx]) for fpr_fold, tpr_fold in zip(fpr_list, tpr_list)], axis=0)\n",
    "    plt.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC', lw=2)\n",
    "\n",
    "    # Calculate and plot the average ROC AUC\n",
    "    roc_auc_fold_list = []\n",
    "    for fold, (tpr_fold, fpr_fold) in enumerate(zip(tpr_list, fpr_list), start=1):\n",
    "        roc_auc = auc(fpr_fold[class_idx], tpr_fold[class_idx])\n",
    "        roc_auc_fold_list.append(roc_auc)\n",
    "\n",
    "    average_roc_auc = np.mean(roc_auc_fold_list)\n",
    "    plt.title(f'ROC Curve for Class: {class_name}\\nAverage AUC: {average_roc_auc:.3f}')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the Mean ROC curve across all classes\n",
    "plt.figure()\n",
    "\n",
    "# ROC curve for each class\n",
    "for class_idx, class_name in enumerate(np.unique(labels)):\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.zeros_like(mean_fpr)\n",
    "\n",
    "    for fold, (tpr_fold, fpr_fold) in enumerate(zip(tpr_list, fpr_list), start=1):\n",
    "        mean_tpr += np.interp(mean_fpr, fpr_fold[class_idx], tpr_fold[class_idx])\n",
    "\n",
    "    mean_tpr /= len(tpr_list)\n",
    "\n",
    "    plt.plot(mean_fpr, mean_tpr, lw=1, label=f'Mean ROC - {class_name}')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Mean ROC Curve Across All Classes and Folds')\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "\n",
    "\n",
    "# Display or use the confusion matrices as needed\n",
    "for fold, conf_matrix in enumerate(conf_matrices, start=1):\n",
    "    plt.figure()\n",
    "\n",
    "    # Use scikit-learn's confusion_matrix function to plot the matrix\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    \n",
    "    plt.title(f'Confusion Matrix - Fold {fold}')\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Add numerical values in each square\n",
    "    for i, class_label in enumerate(np.unique(labels)):\n",
    "        for j, predicted_label in enumerate(np.unique(labels)):\n",
    "            plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center', color='red')\n",
    "\n",
    "    tick_marks = np.arange(len(np.unique(labels)))\n",
    "    plt.xticks(tick_marks, np.unique(labels), rotation=45)\n",
    "    plt.yticks(tick_marks, np.unique(labels))\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Calculate and plot the average confusion matrix\n",
    "average_conf_matrix = np.mean(conf_matrices, axis=0)\n",
    "plt.figure()\n",
    "plt.imshow(average_conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(f'Average Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add numerical values in each square\n",
    "for i, class_label in enumerate(np.unique(labels)):\n",
    "    for j, predicted_label in enumerate(np.unique(labels)):\n",
    "        plt.text(j, i, str(int(average_conf_matrix[i, j])), ha='center', va='center', color='red')\n",
    "\n",
    "tick_marks = np.arange(len(np.unique(labels)))\n",
    "plt.xticks(tick_marks, np.unique(labels), rotation=45)\n",
    "plt.yticks(tick_marks, np.unique(labels))\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()    \n",
    "\n",
    "# Print average confusion matrix\n",
    "print(\"Average Confusion Matrix:\")\n",
    "print(average_conf_matrix)\n",
    "\n",
    "# Print AUC for each class and fold\n",
    "for class_idx, class_name in enumerate(np.unique(labels)):\n",
    "    for fold, roc_auc_fold in enumerate(roc_auc_list, start=1):\n",
    "        print(f'Fold {fold}, Class {class_name}, AUC: {roc_auc_fold[class_idx]}')\n",
    "\n",
    "# Print accuracies for each fold\n",
    "for fold, accuracy in enumerate(accuracies, start=1):\n",
    "    print(f\"Fold {fold} Accuracy: {accuracy}\")\n",
    "\n",
    "# Print average accuracy over all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
